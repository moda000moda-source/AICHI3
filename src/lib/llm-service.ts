/**
 * Local LLM Service
 * Provides integration with local language models like Ollama
 * Supports memory persistence and intelligent conversation
 */

import type { 
  LLMConfig, 
  LLMResponse, 
  OllamaModel, 
  LLMConnectionStatus,
  AIMessage,
  AIMemoryItem,
} from './types';

// Default configuration for local Ollama instance
const DEFAULT_CONFIG: LLMConfig = {
  provider: 'ollama',
  endpoint: 'http://localhost:11434',
  model: 'qwen2.5:7b',
  temperature: 0.7,
  maxTokens: 2048,
  systemPrompt: `ä½ æ˜¯ OmniCore æ™ºèƒ½åŠ©æ‰‹ï¼Œä¸€ä¸ªä¸“ä¸šçš„åŠ å¯†è´§å¸é’±åŒ…ç®¡ç†AIã€‚ä½ å…·å¤‡ä»¥ä¸‹èƒ½åŠ›ï¼š

1. **è®°å¿†èƒ½åŠ›**: ä½ èƒ½è®°ä½ç”¨æˆ·çš„åå¥½ã€äº¤æ˜“å†å²å’Œå¸¸ç”¨æ“ä½œæ¨¡å¼
2. **è¯­è¨€ç†è§£**: ä½ èƒ½ç†è§£ä¸­æ–‡å’Œè‹±æ–‡çš„è‡ªç„¶è¯­è¨€æŒ‡ä»¤
3. **æ§åˆ¶èƒ½åŠ›**: ä½ èƒ½å¸®åŠ©ç”¨æˆ·æ‰§è¡Œé’±åŒ…ç®¡ç†ã€äº¤æ˜“ã€DeFiæ“ä½œç­‰åŠŸèƒ½

å½“ç”¨æˆ·è¯¢é—®é’±åŒ…ç›¸å…³é—®é¢˜æ—¶ï¼Œåˆ†æä»–ä»¬çš„éœ€æ±‚å¹¶æä¾›ä¸“ä¸šå»ºè®®ã€‚
ä¿æŒå‹å¥½ã€ä¸“ä¸šçš„æ€åº¦ï¼Œç”¨ä¸­æ–‡å›å¤ç”¨æˆ·ï¼ˆé™¤éç”¨æˆ·ä½¿ç”¨è‹±æ–‡ï¼‰ã€‚

å½“å‰å¹³å°æ”¯æŒçš„åŠŸèƒ½ï¼š
- å¤šé“¾é’±åŒ…ç®¡ç†ï¼ˆEthereumã€Polygonã€Arbitrumç­‰ï¼‰
- å¤šç­¾äº¤æ˜“å®¡æ‰¹
- DeFiç­–ç•¥ç®¡ç†
- é£é™©è¯„ä¼°
- æ”¯ä»˜ç½‘å…³

è¯·æ ¹æ®ç”¨æˆ·çš„é—®é¢˜æä¾›æœ‰é’ˆå¯¹æ€§çš„å¸®åŠ©ã€‚`,
};

// Storage keys for persistence
const STORAGE_KEYS = {
  CONFIG: 'omnicore_llm_config',
  MESSAGES: 'omnicore_ai_messages',
  MEMORIES: 'omnicore_ai_memories',
};

/**
 * Get stored LLM configuration
 */
export function getLLMConfig(): LLMConfig {
  try {
    const stored = localStorage.getItem(STORAGE_KEYS.CONFIG);
    if (stored) {
      return { ...DEFAULT_CONFIG, ...JSON.parse(stored) };
    }
  } catch (e) {
    console.error('Failed to load LLM config:', e);
  }
  return DEFAULT_CONFIG;
}

/**
 * Save LLM configuration
 */
export function saveLLMConfig(config: Partial<LLMConfig>): void {
  try {
    const current = getLLMConfig();
    localStorage.setItem(STORAGE_KEYS.CONFIG, JSON.stringify({ ...current, ...config }));
  } catch (e) {
    console.error('Failed to save LLM config:', e);
  }
}

/**
 * Get stored conversation history
 */
export function getStoredMessages(): AIMessage[] {
  try {
    const stored = localStorage.getItem(STORAGE_KEYS.MESSAGES);
    if (stored) {
      return JSON.parse(stored);
    }
  } catch (e) {
    console.error('Failed to load messages:', e);
  }
  return [];
}

/**
 * Save conversation history
 */
export function saveMessages(messages: AIMessage[]): void {
  try {
    // Keep last 100 messages for memory efficiency
    const toStore = messages.slice(-100);
    localStorage.setItem(STORAGE_KEYS.MESSAGES, JSON.stringify(toStore));
  } catch (e) {
    console.error('Failed to save messages:', e);
  }
}

/**
 * Get stored AI memories
 */
export function getStoredMemories(): AIMemoryItem[] {
  try {
    const stored = localStorage.getItem(STORAGE_KEYS.MEMORIES);
    if (stored) {
      return JSON.parse(stored);
    }
  } catch (e) {
    console.error('Failed to load memories:', e);
  }
  return [];
}

/**
 * Save AI memories
 */
export function saveMemories(memories: AIMemoryItem[]): void {
  try {
    localStorage.setItem(STORAGE_KEYS.MEMORIES, JSON.stringify(memories));
  } catch (e) {
    console.error('Failed to save memories:', e);
  }
}

/**
 * Add a new memory item
 */
export function addMemory(memory: Omit<AIMemoryItem, 'id' | 'learnedAt' | 'usageCount'>): AIMemoryItem {
  const memories = getStoredMemories();
  const newMemory: AIMemoryItem = {
    ...memory,
    id: `mem-${Date.now()}-${Math.random().toString(36).slice(2, 11)}`,
    learnedAt: Date.now(),
    usageCount: 0,
  };
  memories.push(newMemory);
  saveMemories(memories);
  return newMemory;
}

/**
 * Update memory usage count
 */
export function updateMemoryUsage(memoryId: string): void {
  const memories = getStoredMemories();
  const idx = memories.findIndex(m => m.id === memoryId);
  if (idx !== -1) {
    memories[idx].usageCount += 1;
    saveMemories(memories);
  }
}

/**
 * Check connection to local LLM
 */
export async function checkLLMConnection(config?: LLMConfig): Promise<LLMConnectionStatus> {
  const cfg = config || getLLMConfig();
  
  if (cfg.provider === 'mock') {
    return {
      connected: true,
      provider: 'mock',
      model: 'mock-model',
      lastChecked: Date.now(),
    };
  }
  
  try {
    const response = await fetch(`${cfg.endpoint}/api/tags`, {
      method: 'GET',
      headers: { 'Content-Type': 'application/json' },
    });
    
    if (response.ok) {
      const data = await response.json();
      const models = data.models || [];
      const modelExists = models.some((m: OllamaModel) => m.name.includes(cfg.model.split(':')[0]));
      
      return {
        connected: true,
        provider: cfg.provider,
        model: modelExists ? cfg.model : (models[0]?.name || cfg.model),
        lastChecked: Date.now(),
      };
    }
    
    return {
      connected: false,
      provider: cfg.provider,
      lastChecked: Date.now(),
      error: `HTTP ${response.status}: ${response.statusText}`,
    };
  } catch (e) {
    return {
      connected: false,
      provider: cfg.provider,
      lastChecked: Date.now(),
      error: e instanceof Error ? e.message : 'æ— æ³•è¿æ¥åˆ°æœ¬åœ°æ¨¡å‹æœåŠ¡',
    };
  }
}

/**
 * Get available models from Ollama
 */
export async function getAvailableModels(config?: LLMConfig): Promise<OllamaModel[]> {
  const cfg = config || getLLMConfig();
  
  if (cfg.provider === 'mock') {
    return [{ name: 'mock-model', modifiedAt: new Date().toISOString(), size: 0, digest: 'mock' }];
  }
  
  try {
    const response = await fetch(`${cfg.endpoint}/api/tags`, {
      method: 'GET',
      headers: { 'Content-Type': 'application/json' },
    });
    
    if (response.ok) {
      const data = await response.json();
      return data.models || [];
    }
  } catch (e) {
    console.error('Failed to get models:', e);
  }
  
  return [];
}

/**
 * Build context from memories for the prompt
 */
function buildMemoryContext(memories: AIMemoryItem[]): string {
  if (memories.length === 0) return '';
  
  const relevantMemories = memories
    .filter(m => m.confidence > 0.5)
    .sort((a, b) => b.usageCount - a.usageCount)
    .slice(0, 10);
  
  if (relevantMemories.length === 0) return '';
  
  const memoryText = relevantMemories
    .map(m => `- ${m.key}: ${m.value}`)
    .join('\n');
  
  return `\n\n## ç”¨æˆ·è®°å¿†æ¡£æ¡ˆ\n${memoryText}\n`;
}

/**
 * Build conversation history for context
 */
function buildConversationContext(messages: AIMessage[], maxMessages = 10): string {
  const recent = messages.slice(-maxMessages);
  if (recent.length === 0) return '';
  
  return recent
    .map(m => `${m.role === 'user' ? 'ç”¨æˆ·' : 'AI'}: ${m.content}`)
    .join('\n');
}

/**
 * Generate mock response for testing
 */
function generateMockResponse(input: string, memories: AIMemoryItem[]): string {
  const lowerInput = input.toLowerCase();
  
  // Check memories for personalization
  const userPrefs = memories.filter(m => m.type === 'preference');
  const prefText = userPrefs.length > 0 
    ? `\n\næ ¹æ®æˆ‘çš„è®°å¿†ï¼Œæ‚¨çš„åå¥½æ˜¯ï¼š${userPrefs.map(p => p.value).join('ã€')}`
    : '';
  
  if (lowerInput.includes('é’±åŒ…') || lowerInput.includes('ä½™é¢') || lowerInput.includes('wallet') || lowerInput.includes('balance')) {
    return `æˆ‘å·²ç»æ£€æŸ¥äº†æ‚¨çš„é’±åŒ…çŠ¶æ€ã€‚æ‚¨ç›®å‰æœ‰:

ğŸ’° **æ€»èµ„äº§**: $231,690.75

ä¸»è¦é’±åŒ…:
- Treasury Vault: $125,432 (Ethereum)
- Operating Account: $23,234 (Polygon)
- DeFi Strategy: $8,024 (Arbitrum)

éœ€è¦æˆ‘æ‰§è¡Œä»€ä¹ˆæ“ä½œå—ï¼Ÿ${prefText}`;
  }
  
  if (lowerInput.includes('äº¤æ˜“') || lowerInput.includes('è½¬è´¦') || lowerInput.includes('transaction') || lowerInput.includes('transfer')) {
    return `æˆ‘å¯ä»¥å¸®æ‚¨åˆ›å»ºæ–°äº¤æ˜“ã€‚è¯·æä¾›ä»¥ä¸‹ä¿¡æ¯:

1. å‘é€æ–¹é’±åŒ…
2. æ¥æ”¶åœ°å€
3. é‡‘é¢å’Œä»£å¸
4. äº¤æ˜“æè¿°

æˆ–è€…æ‚¨å¯ä»¥è¯´ "ä»Treasury Vaultè½¬è´¦5000 USDCåˆ°ä¾›åº”å•†"ï¼Œæˆ‘ä¼šè‡ªåŠ¨è§£æã€‚${prefText}`;
  }
  
  if (lowerInput.includes('é£é™©') || lowerInput.includes('åˆ†æ') || lowerInput.includes('risk') || lowerInput.includes('analysis')) {
    return `ğŸ” **é£é™©åˆ†ææŠ¥å‘Š**

å½“å‰å¾…å¤„ç†äº¤æ˜“é£é™©:

âš ï¸ **é«˜é£é™©** - tx-3 (Operating Account)
- å¤§é¢è½¬è´¦: 25,000 USDT
- é¦–æ¬¡æ”¶æ¬¾åœ°å€
- å»ºè®®: éªŒè¯æ”¶æ¬¾æ–¹èº«ä»½

âœ… **ä½é£é™©** - tx-1 (Treasury Vault)
- å·²çŸ¥æ”¶æ¬¾æ–¹
- å¸¸è§„äº¤æ˜“æ¨¡å¼

éœ€è¦æˆ‘æä¾›æ›´è¯¦ç»†çš„åˆ†æå—ï¼Ÿ${prefText}`;
  }
  
  if (lowerInput.includes('defi') || lowerInput.includes('ç­–ç•¥') || lowerInput.includes('æ”¶ç›Š')) {
    return `ğŸ“Š **DeFi ç­–ç•¥å»ºè®®**

åŸºäºæ‚¨çš„é£é™©åå¥½ï¼Œæ¨è:

1. **ç¨³å®šå¸å€Ÿè´·** (Aave V3)
   - APY: 5.2%
   - é£é™©: ä½

2. **ETH è´¨æŠ¼** (Lido)
   - APY: 3.8%
   - é£é™©: ä½

3. **æµåŠ¨æ€§æŒ–çŸ¿** (Uniswap V3)
   - APY: 12.5%
   - é£é™©: ä¸­

éœ€è¦æˆ‘å¸®æ‚¨é…ç½®è‡ªåŠ¨æŠ•èµ„ç­–ç•¥å—ï¼Ÿ${prefText}`;
  }
  
  if (lowerInput.includes('è®°ä½') || lowerInput.includes('è®°å¿†') || lowerInput.includes('remember')) {
    return `å¥½çš„ï¼Œæˆ‘å·²ç»è®°ä½äº†è¿™ä¸ªä¿¡æ¯ï¼æˆ‘çš„è®°å¿†ç³»ç»Ÿä¼šè‡ªåŠ¨å­¦ä¹ æ‚¨çš„åå¥½å’Œæ“ä½œæ¨¡å¼ã€‚

å½“å‰æˆ‘è®°ä½çš„å†…å®¹åŒ…æ‹¬:
${memories.length > 0 ? memories.map(m => `- ${m.key}: ${m.value}`).join('\n') : '- è¿˜æ²¡æœ‰å­¦ä¹ åˆ°ç‰¹å®šåå¥½'}

æ‚¨å¯ä»¥éšæ—¶å‘Šè¯‰æˆ‘éœ€è¦è®°ä½çš„å†…å®¹ï¼`;
  }
  
  if (lowerInput.includes('ä½ å¥½') || lowerInput.includes('hello') || lowerInput.includes('hi')) {
    return `æ‚¨å¥½ï¼æˆ‘æ˜¯ OmniCore æ™ºèƒ½åŠ©æ‰‹ ğŸ¤–

æˆ‘æ˜¯ä¸€ä¸ªå…·å¤‡è®°å¿†ã€è¯­è¨€ç†è§£å’Œæ§åˆ¶èƒ½åŠ›çš„AIæœºå™¨äººï¼Œå¯ä»¥å¸®åŠ©æ‚¨:

â€¢ ğŸ“Š æŸ¥è¯¢å’Œç®¡ç†é’±åŒ…
â€¢ ğŸ’¸ åˆ›å»ºå’Œç­¾ç½²äº¤æ˜“
â€¢ ğŸ” åˆ†æäº¤æ˜“é£é™©
â€¢ ğŸ“ˆ ç®¡ç† DeFi ç­–ç•¥
â€¢ ğŸ§  è®°ä½æ‚¨çš„åå¥½å’Œä¹ æƒ¯

è¯·å‘Šè¯‰æˆ‘æ‚¨éœ€è¦ä»€ä¹ˆå¸®åŠ©ï¼Ÿ${prefText}`;
  }
  
  return `æ„Ÿè°¢æ‚¨çš„æé—®ï¼æˆ‘æ˜¯ OmniCore æ™ºèƒ½åŠ©æ‰‹ï¼Œå¯ä»¥å¸®åŠ©æ‚¨:

â€¢ ğŸ“Š æŸ¥è¯¢å’Œç®¡ç†é’±åŒ…
â€¢ ğŸ’¸ åˆ›å»ºå’Œç­¾ç½²äº¤æ˜“
â€¢ ğŸ” åˆ†æäº¤æ˜“é£é™©
â€¢ ğŸ“ˆ ç®¡ç† DeFi ç­–ç•¥
â€¢ âš™ï¸ é…ç½®å¹³å°è®¾ç½®

è¯·å‘Šè¯‰æˆ‘æ‚¨éœ€è¦ä»€ä¹ˆå¸®åŠ©ï¼Ÿ${prefText}`;
}

/**
 * Stream response from Ollama API
 */
export async function* streamLLMResponse(
  userMessage: string,
  conversationHistory: AIMessage[],
  memories: AIMemoryItem[],
  config?: LLMConfig,
): AsyncGenerator<string, void, unknown> {
  const cfg = config || getLLMConfig();
  
  // Use mock mode if provider is mock or if endpoint is not reachable
  if (cfg.provider === 'mock') {
    const mockResponse = generateMockResponse(userMessage, memories);
    // Simulate streaming by yielding chunks
    const words = mockResponse.split('');
    for (let i = 0; i < words.length; i++) {
      await new Promise(resolve => setTimeout(resolve, 10));
      yield words[i];
    }
    return;
  }
  
  const memoryContext = buildMemoryContext(memories);
  const conversationContext = buildConversationContext(conversationHistory);
  
  const fullPrompt = `${cfg.systemPrompt || DEFAULT_CONFIG.systemPrompt}${memoryContext}

## å¯¹è¯å†å²
${conversationContext}

ç”¨æˆ·: ${userMessage}
AI:`;

  try {
    const response = await fetch(`${cfg.endpoint}/api/generate`, {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({
        model: cfg.model,
        prompt: fullPrompt,
        stream: true,
        options: {
          temperature: cfg.temperature || 0.7,
          num_predict: cfg.maxTokens || 2048,
        },
      }),
    });

    if (!response.ok) {
      throw new Error(`HTTP ${response.status}: ${response.statusText}`);
    }

    const reader = response.body?.getReader();
    if (!reader) {
      throw new Error('No response body');
    }

    const decoder = new TextDecoder();
    
    while (true) {
      const { done, value } = await reader.read();
      if (done) break;
      
      const chunk = decoder.decode(value, { stream: true });
      const lines = chunk.split('\n').filter(line => line.trim());
      
      for (const line of lines) {
        try {
          const data: LLMResponse = JSON.parse(line);
          if (data.response) {
            yield data.response;
          }
        } catch {
          // Skip invalid JSON lines
        }
      }
    }
  } catch (e) {
    console.error('LLM streaming error:', e);
    // Fallback to mock response
    const mockResponse = generateMockResponse(userMessage, memories);
    yield mockResponse;
  }
}

/**
 * Non-streaming API call (for simpler use cases)
 */
export async function generateLLMResponse(
  userMessage: string,
  conversationHistory: AIMessage[],
  memories: AIMemoryItem[],
  config?: LLMConfig,
): Promise<string> {
  let fullResponse = '';
  
  for await (const chunk of streamLLMResponse(userMessage, conversationHistory, memories, config)) {
    fullResponse += chunk;
  }
  
  return fullResponse;
}

/**
 * Extract potential memory from user input
 */
export function extractPotentialMemory(
  userMessage: string,
  aiResponse: string,
): Omit<AIMemoryItem, 'id' | 'learnedAt' | 'usageCount'> | null {
  const lowerInput = userMessage.toLowerCase();
  
  // Detect preference statements
  if (lowerInput.includes('æˆ‘å–œæ¬¢') || lowerInput.includes('æˆ‘åå¥½') || lowerInput.includes('i prefer') || lowerInput.includes('i like')) {
    return {
      type: 'preference',
      key: 'ç”¨æˆ·åå¥½',
      value: userMessage,
      confidence: 0.8,
    };
  }
  
  // Detect contact mentions
  if (lowerInput.includes('åœ°å€') && lowerInput.includes('0x')) {
    const addressMatch = userMessage.match(/0x[a-fA-F0-9]{6,}/);
    if (addressMatch) {
      return {
        type: 'contact',
        key: 'æåŠçš„åœ°å€',
        value: addressMatch[0],
        confidence: 0.7,
      };
    }
  }
  
  // Detect remember requests
  if (lowerInput.includes('è®°ä½') || lowerInput.includes('remember')) {
    return {
      type: 'preference',
      key: 'ç”¨æˆ·è¯·æ±‚è®°å¿†',
      value: userMessage,
      confidence: 0.9,
    };
  }
  
  return null;
}

/**
 * Clear all stored data
 */
export function clearAllData(): void {
  localStorage.removeItem(STORAGE_KEYS.CONFIG);
  localStorage.removeItem(STORAGE_KEYS.MESSAGES);
  localStorage.removeItem(STORAGE_KEYS.MEMORIES);
}
